{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pong Atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xALt5K4YALJN"
   },
   "outputs": [],
   "source": [
    "from agent0 import Agent0\n",
    "from env2 import Env\n",
    "from memory import ReplayMemory\n",
    "import retro\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import trange\n",
    "import collections\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from test import test\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPzn-OcGBAAZ"
   },
   "source": [
    "## Training Pong AI against Pong Bot\n",
    "\n",
    "An AI will be trained to play the ATARI game, Pong. During the start, the AI is trained using Double Deep Q Network (Double DQN) provided from the Rainbow Github by Kaixhin. As stated to be Double Deep Q Network, two neural network will be used to handle the learning process of the AI. A DQN network will be responsible as the main neural network for the selection of the next action with the maximum value. On the other hand, another same structured DQN network usually called Target Network will be responsible for the evaluation of that action. The agent will be trained for 5 million steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEQhRS4a_Uvy"
   },
   "source": [
    "### Loading variables, parameters and environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mv_gaxnT_6V4"
   },
   "source": [
    "The Environment Wrapper (Env) from env2 is used to modify the environment to cater to our training process.\n",
    "- It simplifies the control mapping needed to interact with the environment.\n",
    "- Output 4 stack of states instead of 1 state for the neural network model to process.\n",
    "- A random initial state will be use as the starting state when the environment is reset so that it does not overfit too much to the original starting state.\n",
    "- For every state the model gets from interacting with the environment, the Pong game will be progressed for 4 frames. The aforementioned state will be represented by Max Pooling on the last 2 frames. The reward gained for the state will be the accumulation of the reward gained from the 4 frames.\n",
    "- All of the modifications above are also applicable for a 2-Player Pong environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnXI57h6AM0p"
   },
   "outputs": [],
   "source": [
    "env0 = retro.make(game='Pong-Atari2600', players=1) # Pong environment for 1 Player\n",
    "env = Env(env0) # Environment Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xxx8kjziHect"
   },
   "source": [
    "### Initializing the arguments for the model and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ap2IVFZ-AQ4R"
   },
   "outputs": [],
   "source": [
    "class args_parser():\n",
    "    def __init__(self, device, model=None):\n",
    "        self.atoms = 51\n",
    "        self.V_min = -10\n",
    "        self.V_max = 10\n",
    "        self.batch_size = 32\n",
    "        self.multi_step = 3\n",
    "        self.discount = 0.99\n",
    "        self.norm_clip = 10.0\n",
    "        self.learning_rate = 0.0000625\n",
    "        self.adam_eps = 1.5e-4\n",
    "        self.architecture = \"canonical\"\n",
    "        self.history_length = 4\n",
    "        self.hidden_size = 512\n",
    "        self.noisy_std = 0.1\n",
    "        self.priority_weight = 0.4\n",
    "        self.priority_exponent = 0.5\n",
    "        self.evaluation_episodes = 10\n",
    "        self.render = False\n",
    "        self.players = 1\n",
    "        self.device = device\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMg1iv7kAbST"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gxt0H96fAinz"
   },
   "outputs": [],
   "source": [
    "env.training # No purpose\n",
    "args = args_parser(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaeqUEJeApPq"
   },
   "outputs": [],
   "source": [
    "CAPACITY = int(1e6)\n",
    "EVALUATION_SIZE = 500\n",
    "PRIO_WEIGHTS = 0.4\n",
    "STEPS = int(5e6)\n",
    "REPLAY_FREQ = 4\n",
    "REWARD_CLIP = 1\n",
    "LEARNING_START = int(10e3)\n",
    "EVALUATION_INTERVAL = 100000\n",
    "CHECKPOINT_INTERVAL = 1000000\n",
    "TARGET_UPDATE = int(8e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfU0Lb1QAq_I"
   },
   "outputs": [],
   "source": [
    "model = Agent0(args, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAd-JlW__sxr"
   },
   "source": [
    "### Training Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EjuUvuXAuBS"
   },
   "outputs": [],
   "source": [
    "priority_weight_increase = (1 - PRIO_WEIGHTS) / (STEPS - LEARNING_START)\n",
    "mem = ReplayMemory(args, CAPACITY)\n",
    "val_mem = ReplayMemory(args, EVALUATION_SIZE)\n",
    "metrics = {'steps': [], 'rewards': [], 'Qs': [], 'best_avg_reward': -float('inf')}\n",
    "results_dir = os.getcwd()\n",
    "model.train()\n",
    "done = True\n",
    "\n",
    "for T in trange(1, STEPS+1):\n",
    "    if done:\n",
    "        state = env.reset().to(device)\n",
    "\n",
    "    if T % REPLAY_FREQ == 0:\n",
    "        model.reset_noise()\n",
    "\n",
    "    action = model.act(state)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    if REWARD_CLIP > 0:\n",
    "        reward = max(min(reward, REWARD_CLIP), -REWARD_CLIP)\n",
    "    mem.append(state, action, reward, done)\n",
    "\n",
    "    if T >= LEARNING_START:\n",
    "        mem.priority_weight = min(mem.priority_weight + priority_weight_increase, 1)  # Anneal importance sampling weight β to 1\n",
    "\n",
    "        if T % REPLAY_FREQ == 0:\n",
    "            model.learn(mem)  # Train with n-step distributional double-Q learning\n",
    "\n",
    "        if T % EVALUATION_INTERVAL == 0:\n",
    "            model.eval()  # Set DQN (online network) to evaluation mode\n",
    "            avg_reward, avg_Q = test(args, T, model, val_mem, metrics, results_dir)  # Test\n",
    "            model.train()  # Set DQN (online network) back to training mode\n",
    "\n",
    "        # Update target network\n",
    "        if T % TARGET_UPDATE == 0:\n",
    "            model.update_target_net()\n",
    "\n",
    "        # Checkpoint the network\n",
    "        if (CHECKPOINT_INTERVAL != 0) and (T % CHECKPOINT_INTERVAL == 0):\n",
    "            model.save(results_dir, 'checkpoint.pth')\n",
    "\n",
    "    state = next_state.to(device)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result from Training Pong AI against Pong Bot\n",
    "\n",
    "The following figure below showcase the Q Value Graph during the training process which represents the expected total reward from the behaviour of the agent.\n",
    "\n",
    "<img src=\"./assets/Q Value.png\" width=\"1000\"/>\n",
    "\n",
    "During the first 1 million steps, the Q Value of the agent is decreasing alot as we assumed it is still learning the correct moves for every possible state. Throughout the later steps. the Q Value is gradually increasing, indicating that the agent is slowly improving and optimizing its action for a given state. The lowest Q Value obtained was at the 900th episode at -1.887. The highest Q value obtained was at the 4.9 millionth episode at 0.219."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next figure showcase the Reward Graph obtained from the evaluation test-run on the Agent Neural Network every 100000 intervals\n",
    "\n",
    "<img src=\"./assets/Reward.png\" width=\"1000\"/>\n",
    "\n",
    "For every test-run, the agent is evaluated by playing against the Pong Bot for 10 rounds. If the agent managed to score a point, it will gain a reward of +1 whereas if the Pong Bot scores a point, the agent will be penalised with a reward of -1. The dotted lines in the graph represent the maximum total reward and minimum total reward it managed to obtain within the 10 rounds played. The darker blue line showcase the mean reward it obtained within the 10 games while the shaded light blue areas are the variance of the reward by a standard deviation of 1. From the graph, we can see that the agent is slowly exploring the game from the start and it starts to learn to play the game approximately starting at 1 million steps till the 2.3 million steps. From then on, the agent is just optimizing itself to maximize the rewards that it can gained. In one of the test-run conducted on the 4.8 million steps, the agent showcase that it managed to beat the Pong Bot with a maximum reward of 21 for 10 games with no variance in each games at all and we believe the agent neural network has reached the optimal point in tuning itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the Pong AI against itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/Test2.gif\" >\n",
    "\n",
    "An evaluation was conducted using the trained Pong AI against itself and was found that the Pong AI controlling the left paddle are more prone to losing as it cannot deflect the ball properly. We tried to solve this issue and found that there is a difference in the state given if the Pong AI were to play on the right side compared to the left side. When the ball veered to the right side during the start of the game, the ball would be able to reach the outside boundary to score without bouncing off the floor or the ceiling of the game. However, when the ball veered to the left side during the start of the game, the ball would hit the floor or the ceiling of the game before even reaching to the outside boundary. This difference in the state is what cause the Pong AI to not be able to perform well on the left side as it is only trained on the right side and we felt that it might had overfitted on playing for the right paddle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BrI-IQHBWyb"
   },
   "source": [
    "## Training Pong AI against itself\n",
    "\n",
    "In this section, we will be further fine-tuning the Pong AI by training the neural network against itself. All of the parameters and model used will be the same as the ones used to train the Pong AI against the the Pong Bot. The only difference is the environment will be set for 2-Players and during the training process, two neural networks will be trained; one on the left paddle and another on the right paddle. Both agents will be trained for 3 million steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuIttPnzCvWg"
   },
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "from agent2 import Agent2\n",
    "from test2 import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbzhgxc9BWJq"
   },
   "outputs": [],
   "source": [
    "env0 = retro.make(game='Pong-Atari2600', players=2)\n",
    "env = Env(env0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjm7vajTCA9a"
   },
   "outputs": [],
   "source": [
    "trained = 'model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_s7OHIAzBhbc"
   },
   "outputs": [],
   "source": [
    "env.train() # No purpose\n",
    "args = args_parser(device, trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ih34-CdrBjuH"
   },
   "outputs": [],
   "source": [
    "model = Agent(args, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cp8_5VHfDNoD"
   },
   "outputs": [],
   "source": [
    "model2 = Agent2(args, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSEu2iXdDQvy"
   },
   "outputs": [],
   "source": [
    "CAPACITY = int(1e4)\n",
    "EVALUATION_SIZE = 500\n",
    "PRIO_WEIGHTS = 0.4\n",
    "STEPS = int(3e6)\n",
    "REPLAY_FREQ = 4\n",
    "REWARD_CLIP = 1\n",
    "LEARNING_START = int(10e3)\n",
    "EVALUATION_INTERVAL = 100000\n",
    "TARGET_UPDATE = int(8e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5U2c9LpRDVIO"
   },
   "outputs": [],
   "source": [
    "priority_weight_increase = (1 - PRIO_WEIGHTS) / (STEPS - LEARNING_START)\n",
    "mem = ReplayMemory(args, CAPACITY)\n",
    "mem2 = ReplayMemory(args, CAPACITY)\n",
    "metrics = {'steps': [], 'reward1': [], 'reward2': [], 'best_avg_reward1': -float('inf'), 'best_avg_reward2': -float('inf')}\n",
    "results_dir = os.getcwd()\n",
    "model.train()\n",
    "model2.train()\n",
    "done = True\n",
    "\n",
    "for T in trange(1, STEPS+1):\n",
    "    if done:\n",
    "        state = env.reset().to(device)\n",
    "\n",
    "    if T % REPLAY_FREQ == 0:\n",
    "        model.reset_noise()\n",
    "        model2.reset_noise()\n",
    "\n",
    "    state2 = torch.flip(state,[2])\n",
    "    action = model.act(state)\n",
    "    action2 = model2.act(state2)\n",
    "    next_state, reward, done, info = env.step_2P(action, action2)\n",
    "\n",
    "    reward1, reward2 = reward\n",
    "    mem.append(state, action, reward1, done)\n",
    "    mem2.append(state2, action2, reward2, done)\n",
    "\n",
    "    if T >= LEARNING_START:\n",
    "        mem.priority_weight = min(mem.priority_weight + priority_weight_increase, 1)  # Anneal importance sampling weight β to 1\n",
    "        mem2.priority_weight = min(mem2.priority_weight + priority_weight_increase, 1)\n",
    "\n",
    "        if T % REPLAY_FREQ == 0:\n",
    "            model.learn(mem)  # Train with n-step distributional double-Q learning\n",
    "            model2.learn(mem2)\n",
    "\n",
    "        if T % EVALUATION_INTERVAL == 0:\n",
    "            model.eval()  # Set DQN (online network) to evaluation mode\n",
    "            model2.eval()\n",
    "            test(args, T, model, model2, env0, metrics, results_dir)  # Test\n",
    "            model.train()  # Set DQN (online network) back to training mode\n",
    "            model2.train()\n",
    "\n",
    "        # Update target network\n",
    "        if T % TARGET_UPDATE == 0:\n",
    "            model.update_target_net()\n",
    "            model2.update_target_net()\n",
    "\n",
    "\n",
    "    state = next_state.to(device)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result from Training Pong AI against itself\n",
    "\n",
    "<table><tr><td><img src='./assets/Reward1.png'></td><td><img src='./assets/Reward2.png'></td></tr></table>\n",
    "\n",
    "Two graphs represented above refers to the reward score of the <strong>Right</strong> agent (Reward-1) and the <strong>Left</strong> agent (Reward-2) respectively. Before we start the training process, we hypothesize that for the Pong AI to fine-tune itself, the opposing AI must be able to win against the Pong AI so that the Pong AI can improve by training against the improved opposing AI and the reward graph should be a zig-zag line. As for reaching the optimal point, both agents would need to have an expected reward score of 0 to indicate that both agents are fine-tuned to the max. As shown in the graph, we can see that the <strong>Right</strong> agent is consistently winning more against the <strong>Left</strong> agent. However, there are two instances (1.3 million steps and 2.4 million steps) where the <strong>Left</strong> agent managed to win against the <strong>Right</strong> agent, indicating that it has improved and hence the <strong>Right</strong> agent is able to improve more following these two instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Space Invaders Atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import retro\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import imageio\n",
    "from env_SI import Env\n",
    "from collections import deque, namedtuple\n",
    "from torch.autograd import Variable\n",
    "from tqdm import trange\n",
    "import random\n",
    "from test import test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Deep Q Network for Space Invaders\n",
    "\n",
    "A simple neural network constructed for Deep Q Network are as follows:\n",
    "- A convolutional layer which takes in 4 stack of states from the environment.\n",
    "- Another convolutional layer to further conduct features extraction.\n",
    "- The features are flatten as a linear layer and forward to another linear layer.\n",
    "- The last linear layer will be softmaxed to decide the action to be taken by the model.\n",
    "\n",
    "The agent will be trained for 1 million steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Space_Invaders_DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Space_Invaders_DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 16, 8, stride=4) # output will be 20x20x16\n",
    "        self.conv2 = nn.Conv2d(16, 32, 4, stride=2) # output will be 9x9x32\n",
    "        self.fc1 = nn.Linear(32*81, 256)\n",
    "        self.fc2 = nn.Linear(256, 6)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 32*81)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory\n",
    "\n",
    "Stores the agent's experience in each timestep and will be utilized to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen = capacity)\n",
    "\n",
    "    def store(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]\n",
    "\n",
    "    def length(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Network Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, args):\n",
    "        self.batch_size = args.batch_size\n",
    "        self.gamma = args.gamma\n",
    "        self.loss_fn = args.loss_fn\n",
    "\n",
    "        self.online_net = Space_Invaders_DQN().to(device=args.device)\n",
    "        if args.model:  # Load pretrained model if provided\n",
    "            if os.path.isfile(args.model):\n",
    "                state_dict = torch.load(args.model, map_location='cpu')  # Always load tensors onto CPU by default, will shift to GPU if necessary\n",
    "                if 'conv1.weight' in state_dict.keys():\n",
    "                    for old_key, new_key in (('conv1.weight', 'convs.0.weight'), ('conv1.bias', 'convs.0.bias'), ('conv2.weight', 'convs.2.weight'), ('conv2.bias', 'convs.2.bias'), ('conv3.weight', 'convs.4.weight'), ('conv3.bias', 'convs.4.bias')):\n",
    "                        state_dict[new_key] = state_dict[old_key]  # Re-map state dict for old pretrained models\n",
    "                        del state_dict[old_key]  # Delete old keys for strict load_state_dict\n",
    "                self.online_net.load_state_dict(state_dict)\n",
    "                print(\"Loading pretrained model: \" + args.model)\n",
    "            else:  # Raise error if incorrect model path provided\n",
    "                raise FileNotFoundError(args.model)\n",
    "    \n",
    "        self.optimizer = optim.Adam(self.online_net.parameters(), lr=args.lr)\n",
    "    \n",
    "    \n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            return self.online_net(state.unsqueeze(0)).argmax(1).item()\n",
    "    \n",
    "    def learn(self, mem):\n",
    "        # check if enough experience collected so far\n",
    "        # the agent continues with a random policy without updates till then\n",
    "        if mem.length() < self.batch_size:\n",
    "            return\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        # sample a random batch from the replay memory to learn from experience\n",
    "        # for no experience replay the batch size is 1 and hence learning online\n",
    "        transitions = mem.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        # isolate the values\n",
    "        non_terminal_mask = np.array(list(map(lambda s: s is not None, batch.next_state)))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_next_state = Variable(torch.cat([s for s in batch.next_state if s is not None]))\n",
    "\n",
    "        batch_state = Variable(torch.cat(batch.state)).to(device)\n",
    "        batch_action = Variable(torch.stack(batch.action)).to(device)\n",
    "        batch_reward = Variable(torch.stack(batch.reward)).to(device)\n",
    "    \n",
    "        # There is no separate target Q-network implemented and all updates are done\n",
    "        # synchronously at intervals of 1 unlike in the original paper\n",
    "        # current Q-values\n",
    "        current_Q = self.online_net(batch_state).gather(1, batch_action)\n",
    "        # expected Q-values (target)\n",
    "        max_next_Q = self.online_net(batch_next_state).detach().max(1)[0]\n",
    "        expected_Q = batch_reward\n",
    "        \n",
    "        expected_Q[non_terminal_mask] += (self.gamma * max_next_Q).data.unsqueeze(1)\n",
    "        # with torch.no_grad():\n",
    "        #     expected_Q = Variable(torch.from_numpy(expected_Q).cuda())\n",
    "    \n",
    "        # loss between current Q values and target Q values\n",
    "        if self.loss_fn == 'l1':\n",
    "            loss = F.smooth_l1_loss(current_Q, expected_Q)\n",
    "        else:\n",
    "            loss = F.mse_loss(current_Q, expected_Q)\n",
    "    \n",
    "        # backprop the loss\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self):\n",
    "        self.online_net.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.online_net.eval()\n",
    "\n",
    "    def save(self, path, name='model.pth'):\n",
    "        torch.save(self.online_net.state_dict(), os.path.join(path, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Environment Wrapper (Env) from env_SI is used to modify the environment to cater to the Space Invaders\n",
    "- Output 4 stack of states instead of 1 state for the neural network model to process.\n",
    "- A random initial state will be use as the starting state when the environment is reset so that it does not overfit too much to the original starting state.\n",
    "- For every state the model gets from interacting with the environment, the Space Invaders game will be progressed for 4 frames. The aforementioned state will be represented by using the last frame. The reward gained for the state will be the accumulation of the reward gained from the 4 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env0 = gym.make('SpaceInvaders-v0')\n",
    "env = Env(env0, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the arguments for the model and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args_parser():\n",
    "    def __init__(self, device, model=None):\n",
    "        self.batch_size = 32\n",
    "        self.lr = 0.0001\n",
    "        self.gamma = 0.99\n",
    "        self.loss_fn = 'l1'\n",
    "        self.evaluation_episodes = 5\n",
    "        self.device = device\n",
    "        self.model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "\n",
    "results_dir = os.getcwd()\n",
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPACITY = int(1e4)\n",
    "STEPS = int(1e6)\n",
    "EVALUATION_INTERVAL = int(1e5)\n",
    "REPLAY_FREQ = 4\n",
    "EPSILON_START = 0.95\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY = 600000\n",
    "args = args_parser(device)\n",
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQNAgent(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = RepMemory(CAPACITY)\n",
    "metrics = {'steps': [], 'rewards': [], 'best_avg_reward': -float('inf')}\n",
    "done = True\n",
    "model.train()\n",
    "\n",
    "for T in trange(1, STEPS+1):\n",
    "    if done:\n",
    "        state = env.reset().to(device)\n",
    "\n",
    "    if T < (EPSILON_DECAY + 1):\n",
    "        eps_threshold = T * ((EPSILON_END - EPSILON_START) / (EPSILON_DECAY)) + EPSILON_START\n",
    "    else:\n",
    "        eps_threshold = 0.05\n",
    "\n",
    "    if random.random() > eps_threshold:\n",
    "        action = model.act(state)\n",
    "    else:\n",
    "        action = random.randint(0, 5)\n",
    "\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    mem.store((state.unsqueeze(0), torch.tensor([action]), next_state.unsqueeze(0), torch.tensor([reward])))\n",
    "\n",
    "    if T % REPLAY_FREQ == 0:\n",
    "        model.learn(mem)\n",
    "\n",
    "    if T % EVALUATION_INTERVAL == 0:\n",
    "        model.eval()\n",
    "        gif = test(args, T, model, env0, metrics, results_dir)\n",
    "        imageio.mimsave(os.path.join(results_dir, './GIF/DQN{}.gif'.format(i)), gif)\n",
    "        i += 1\n",
    "        model.train()\n",
    "        \n",
    "    state = next_state.to(device)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial-And-Error and Results from Training Space Invaders on Deep Q Network\n",
    "\n",
    "<img src=\"./assets/FirstTest.gif\" >\n",
    "\n",
    "During the first training process of the Space Invaders AI, the AI will maximize its reward by consistently fire lasers to eliminate the invaders. However, we felt that the AI is not learning the game properly yet as the spaceship only randomly shoots at the aliens to try to gain rewards from it. Hence, a small change is made to the environment wrapper by penalizing the agent with a reward of -100 to force the agent to move the spaceship and dodge the lasers shot by the invaders.\n",
    "\n",
    "### After small changes in the environment\n",
    "\n",
    "<img src=\"./assets/DQN.gif\" >\n",
    "\n",
    "Now the Space Invaders AI can learn to dodge the lasers shot by the invaders instead of the spaceship randomly moving around and shoot its lasers.\n",
    "\n",
    "<img src=\"./assets/SI_DQN.png\" >\n",
    "\n",
    "The agent is evaluated every 100000 steps and the rewards obtained is represented on the graph above. Even though we have tune the environment for the Space Invaders AI to learn to dodge the lasers, it does not seem to be able to learn exceptionally well. This may be due to the issue of the Maximization Bias in Deep Q Network where it has the tendency to overestimate both the value and the action-value (Q) functions. Hence, we will try to use a Double Deep Q Network to train the AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Double Deep Q Network for Space Invaders\n",
    "\n",
    "For the Double Deep Q Network, we will use the same model used to train the Pong AI from the Rainbow Github by Kaixhin. All the parameters set is also the same as the default parameters set to train the Pong AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "from memory import ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args_parser2():\n",
    "    def __init__(self, device, model=None):\n",
    "        self.atoms = 51\n",
    "        self.V_min = -10\n",
    "        self.V_max = 10\n",
    "        self.batch_size = 32\n",
    "        self.multi_step = 3\n",
    "        self.discount = 0.99\n",
    "        self.norm_clip = 10.0\n",
    "        self.learning_rate = 0.0000625\n",
    "        self.adam_eps = 1.5e-4\n",
    "        self.architecture = \"canonical\"\n",
    "        self.history_length = 4\n",
    "        self.hidden_size = 512\n",
    "        self.noisy_std = 0.1\n",
    "        self.priority_weight = 0.4\n",
    "        self.priority_exponent = 0.5\n",
    "        self.evaluation_episodes = 5\n",
    "        self.render = False\n",
    "        self.players = 1\n",
    "        self.device = device\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPACITY = int(1e4)\n",
    "EVALUATION_SIZE = 500\n",
    "PRIO_WEIGHTS = 0.4\n",
    "STEPS = int(1e6)\n",
    "REPLAY_FREQ = 4\n",
    "REWARD_CLIP = 1\n",
    "LEARNING_START = int(10e3)\n",
    "EVALUATION_INTERVAL = 100000\n",
    "TARGET_UPDATE = int(8e3)\n",
    "EPSILON_START = 0.95\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY = 600000\n",
    "i = 1\n",
    "args = args_parser2(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Agent(args, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_weight_increase = (1 - PRIO_WEIGHTS) / (STEPS - LEARNING_START)\n",
    "mem2 = ReplayMemory(args, CAPACITY)\n",
    "metrics2 = {'steps': [], 'rewards': [], 'Qs': [], 'best_avg_reward': -float('inf')}\n",
    "results_dir = os.getcwd()\n",
    "model2.train()\n",
    "done = True\n",
    "\n",
    "for T in trange(1, STEPS+1):\n",
    "    if done:\n",
    "        state = env.reset().to(device)\n",
    "\n",
    "    if T % REPLAY_FREQ == 0:\n",
    "        model2.reset_noise()\n",
    "\n",
    "    if T < (EPSILON_DECAY + 1):\n",
    "        eps_threshold = T * ((EPSILON_END - EPSILON_START) / (EPSILON_DECAY)) + EPSILON_START\n",
    "    else:\n",
    "        eps_threshold = 0.05\n",
    "\n",
    "    action = model2.act_e_greedy(state, eps_threshold)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    if REWARD_CLIP > 0:\n",
    "        reward = max(min(reward, REWARD_CLIP), -REWARD_CLIP)\n",
    "    mem2.append(state, action, reward, done)\n",
    "\n",
    "    if T >= LEARNING_START:\n",
    "        mem2.priority_weight = min(mem2.priority_weight + priority_weight_increase, 1)  # Anneal importance sampling weight β to 1\n",
    "\n",
    "        if T % REPLAY_FREQ == 0:\n",
    "            model2.learn(mem2)  # Train with n-step distributional double-Q learning\n",
    "\n",
    "        if T % EVALUATION_INTERVAL == 0:\n",
    "            model2.eval()  # Set DQN (online network) to evaluation mode\n",
    "            gif = test(args, T, model2, env0, metrics2, results_dir)\n",
    "            imageio.mimsave(os.path.join(results_dir, './GIF/DDQN{}.gif'.format(i)), gif)\n",
    "            i += 1\n",
    "            model2.train()  # Set DQN (online network) back to training mode\n",
    "            \n",
    "        # Update target network\n",
    "        if T % TARGET_UPDATE == 0:\n",
    "            model2.update_target_net()\n",
    "\n",
    "    state = next_state.to(device)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results from Training Space Invaders on Double Deep Q Network\n",
    "\n",
    "<img src=\"./assets/DDQN.gif\" >\n",
    "\n",
    "After training the Space Invaders AI on a Double Deep Q Network, the AI has learn some tricks in playing the game. The AI Agent is able to utilize the shield in the game to block off the lasers attack by the invaders and it also learned to create small openings in the shield to shoot the invaders through it. This shows that utilizing a Double Deep Q Network, it has effectively let the agent to learn the hidden techniques that it can make use of in playing the game.\n",
    "\n",
    "<img src=\"./assets/SI_DDQN.png\" >\n",
    "\n",
    "The agent is also evaluated during the training process in playing 5 rounds of Space Invaders and the graph above represents the expected reward of the agent. From the graph, we can see that the AI agent can perform exceptionally better compared to the AI trained on a Deep Q Network after training it for 1 million steps."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
